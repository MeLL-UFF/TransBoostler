
from gensim.test.utils import datapath, get_tmpfile
from ekphrasis.classes.segmenter import Segmenter
from gensim.models import KeyedVectors, Word2Vec
from collections import OrderedDict
import parameters as params
from scipy import spatial
from tqdm import tqdm
import utils as utils
import pandas as pd
import numpy as np
import fasttext
import operator
import sys
import os
import re

class Transfer:

  def __init__(self):
    pass

  def single_array(self, temp, method):
    """
        Turn vectors into a single array

        Args:
            temp(array): an array containing word embeddings
        Returns:
            a single array generated by the method in 'method'
    """

    if(method == 'AVG'):
      predicate = self.get_arrays_avg(temp)
    elif(method == 'MAX'):
      predicate = max(temp, key=operator.methodcaller('tolist'))
    elif(method == 'MIN'):
      predicate = min(temp, key=operator.methodcaller('tolist'))
    elif(method == 'CONCATENATE'):
      predicate = np.concatenate(temp)
    return predicate

  def get_arrays_avg(self, data):
    """
        Calculate the average fo word embeddings element-wise

        Args:
            data(array): an array containing word embeddings
        Returns:
            a single array generated by taking the element-wise average
    """

    N = len(data)
    avg = np.array(data[0])
    for i in range(1, N):
      avg += np.array(data[i])
    return np.divide(avg, N)

  def build_fasttext_array(self, data, model, method=None):
    """
        Turn relations into a single array

        Args:
            data(array): an array containing all predicates
            model(object): fasttext embedding model
            method(str): method to compact arrays of embedded words
       Returns:
            a dictionary that the keys are the words and the values are single arrays of embeddings
    """

    # segmenter using the word statistics from Wikipedia
    seg = Segmenter(corpus="english")

    dict = {}
    for example in tqdm(data):
      temp = []

      # Tokenize words of relation
      predicate = seg.segment(example[0])
      for word in predicate.split():
        try:
          temp.append(model.get_word_vector(word.lower().strip()))
        except:
          print('Word {} not present in pre-trained model'.format(word.lower().strip()))
          temp.append([0] * params.EMBEDDING_DIMENSION)

      predicate = temp.copy()
      if(method):
        predicate = self.single_array(temp, method)

      if(example[2] == ''):
        example.remove('')
      dict[example[0].rstrip()] = [predicate, example[1:]]
    return dict

  def build_word2vec_array(self, data, model, method='AVG'):
    """
        Turn relations into a single array

        Args:
            data(array): an array containing all predicates
            model(object): word2vec embedding model
            method(str): method to compact arrays of embedded words
       Returns:
            a dictionary that the keys are the words and the values are single arrays of embeddings
    """

    dict = {}
    for example in tqdm(data):
      temp = []

      # Tokenize words of relation
      predicate = seg.segment(example[0])
      for word in predicate.split():
        try:
          temp.append(model.wv[word.lower().strip()])
        except:
          print('Word {} not present in pre-trained model'.format(word.lower().strip()))
          temp.append([0] * params.EMBEDDING_DIMENSION)
    
      predicate = self.single_array(temp, method)

      if(example[2] == ''):
        example.remove('')
      dict[example[0].rstrip()] = [predicate, example[1:]]
    return dict

  def similarity_word2vec(self, source, target, word2vecModel, method):
    """
        Embed relations using pre-trained word2vec

        Args:
            source(array): all predicates from source dataset
            target(array): all predicates from target dataset
            word2vecModel(str): word2vec loaded model
            method(str): method used to compact arrays
       Returns:
             a pandas dataframe containing every pair (source, target) similarity
    """

    # Load Google's pre-trained Word2Vec model.
    #logging.info('Loading pre-trained Word2Vec model')
    #word2vecModel = KeyedVectors.load_word2vec_format(model_path, binary=True)

    source = self.build_word2vec_array(source, word2vecModel, method=method)
    target = self.build_word2vec_array(target, word2vecModel, method=method)

    return self.get_cosine_similarities(source, target)

  def similarity_fasttext(self, sources, targets, fastTextModel, method):
    """
        Embed relations using pre-trained fasttext

        Args:
            sources(array): all predicates from source dataset
            targets(array): all predicates from target dataset
            fastTextModel(str): fastTextModel loaded model
            method(str): method used to compact arrays
       Returns:
            a pandas dataframe containing every pair (source, target) similarity
    """

    # Load Wikipedia's pre-trained fastText model.
    #logging.info('Loading pre-trained fastText model.')
    #fastTextModel = fasttext.load_model(params.WIKIPEDIA_FASTTEXT_PATH)

    sources = self.build_fasttext_array(sources, fastTextModel, method=method)
    targets = self.build_fasttext_array(targets, fastTextModel, method=method)

    return self.get_cosine_similarities(sources, targets)

  def distance_word2vec(self, sources, targets, word2vecModel, method):
    """
        Calculates similarity using Word Mover's Distance for Word2vec Embeddings

        Args:
            sources(array): all predicates from source dataset
            targets(array): all predicates from target dataset
            word2vecModel(str): word2vec loaded model
            method(str): method used to compact arrays
       Returns:
            a pandas dataframe containing every pair (source, target) distance
    """

    sources = utils.build_triples(sources)
    targets = utils.build_triples(targets)

    return self.get_wmd_similarities(sources, targets, word2vecModel)


  def distance_fastText(self, sources, targets, fastTextModel, method):
    """
        Calculates similarity using Word Mover's Distance for fastText Embeddings

        Args:
            sources(array): all predicates from source dataset
            targets(array): all predicates from target dataset
            fastTextModel(str): fastText loaded model
            method(str): method used to compact arrays
       Returns:
            a pandas dataframe containing every pair (source, target) distance
    """

    sources = utils.build_triples(sources)
    targets = utils.build_triples(targets)

    return self.get_wmd_similarities(sources, targets, fastTextModel)

  def map_predicates(self, sources, similarity, searchArgPermutation=False, allowSameTargetMap=False):
      """
        Sorts dataframe to obtain the closest target to a given source

        Args:
            source(array): all predicates from source dataset
            similarity(dataframe): a pandas dataframe containing every pair (source, target) similarity
        Returns:
            a dictionary containing all predicates mapped
      """

      target_mapped, mapping = [], {}
      indexes = similarity.index.tolist()
      utils.write_to_file(indexes, 'similarities.txt')
      
      for index in tqdm(indexes):
        index = re.split(r',\s*(?![^()]*\))', index)
        source, target = index[0].rstrip(), index[1].rstrip()

        if(source in mapping or source not in sources):
          continue

        if(allowSameTargetMap):
          mapping[source] = target
        else:
          if(target in target_mapped):
            continue
          else:
            mapping[source] = target
            target_mapped.append(target)

        if(len(mapping) == len(sources)):
          # All sources mapped to a target
          break

      # Adds source predicates to be mapped to 'empty'
      for s in sources:
        if(s not in mapping):
          mapping[s] = ''

      del indexes
      return mapping

  def write_to_file_closest_distance(self, from_predicate, to_predicate, arity, mapping, filename, recursion=False, searchArgPermutation=False, searchEmpty=False, allowSameTargetMap=False):
    """
          Sorts dataframe to obtain the closest target to a given source

          Args:
              from_predicate(str): predicate of model trained using source data
              to_predicate(str): predicate of model to be trained transfering the structure of source model
              arity(int): arity of from and to predicate
              mapping(dict): a dictionary a pair of mapping (source, target)
         Returns:
              writes a file containing transfer information
    """
    with open(params.TRANSFER_FILENAME, 'w') as file:
      for source in mapping.keys():
        if(mapping[source] != ''):
          file.write((source.replace('`', '') + ': ' +  mapping[source]).replace('`', ''))
        else:
          file.write((source.replace('`', '') + ':'))
        file.write('\n')

      if(recursion):
          file.write('recursion_' + from_predicate + '(A,B): recursion_' + to_predicate + '(A,B)\n')
      file.write('setMap:' + from_predicate + '(' + ','.join([chr(65+i) for i in range(arity)]) + ')' + ',' + to_predicate + '(' + ','.join([chr(65+i) for i in range(arity)]) + ')' + '\n')
      #file.write('setParam:searchArgPermutation=' + str(searchArgPermutation).lower() + '.\n')
      #file.write('setParam:searchEmpty=' + str(searchEmpty).lower() + '.\n')
      file.write('setParam:allowSameTargetMap=' + str(allowSameTargetMap).lower() + '.\n')

    with open(filename + '/transfer.txt', 'w') as file:
      for source in mapping.keys():
        if(mapping[source] != ''):
          file.write((source.replace('`', '') + ': ' +  mapping[source]).replace('`', ''))
        else:
          file.write((source.replace('`', '') + ':'))
        file.write('\n')

      if(recursion):
          file.write('recursion_' + from_predicate + '(A,B): recursion_' + to_predicate + '(A,B)\n')
      file.write('setMap:' + from_predicate + '(' + ','.join([chr(65+i) for i in range(arity)]) + ')' + ',' + to_predicate + '(' + ','.join([chr(65+i) for i in range(arity)]) + ')' + '\n')
      #file.write('setParam:searchArgPermutation=' + str(searchArgPermutation).lower() + '.\n')
      #file.write('setParam:searchEmpty=' + str(searchEmpty).lower() + '.\n')
      file.write('setParam:allowSameTargetMap=' + str(allowSameTargetMap).lower() + '.\n')